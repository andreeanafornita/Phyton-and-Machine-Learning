Data preparation
a) Libraries must be included first
import numpy as np # for linear algebra
import pandas as pd # for data processing, for data reading etc
import matplotlib.pyplot as plt # for plotting
import seaborn as sns # for plotting
import missingno as msno # for outliers, plots
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score,roc_auc_score,roc_curve
b) We are importing the database needed
path="C:\\Users\\nafor\\OneDrive\\Desktop\\proiect practica\\dataset.csv"
data = pd.read_csv(path)
data.info()
c) We search for a variable to analyze
Has it missing values? We need to find out this
data['Attrition_Flag'].isnull().sum()
data["Attrition_Flag"].value_counts()
We change labels of variable "Attrition Flag" to a numeric variable
data["Attrition_Flag"] = data["Attrition_Flag"].map({"Existing Customer":0,"Attrited Customer":1})
data["Attrition_Flag"].value_counts()
d)We replace missing data with "Unknown"
data["Income_Category"] = data["Income_Category"].fillna("Unknown")
data["Marital_Status"] = data["Marital_Status"].fillna("Unknown")
data["Education_Level"] = data["Education_Level"].fillna("Unknown")
e) We replace values that aren't valid (for example abc that doesn't mean anything useful) with "Unknown"
data["Income_Category"].value_counts()/len(data)*100
data["Income_Category"] = data["Income_Category"].replace('abc', 'Unknown')
data["Income_Category"].value_counts()
f) We check if there are more categorical columns without values to replace them
data.isnull().sum()
g) Now, we are selecting the meaningful variables that are CATEGORICAL variables
categorical_columns = [col for col in data.columns if data[col].dtypes == "object"]
print("There are", len(categorical_columns),"categorical variables. These are", categorical_columns)
h)To have a more accurate idea for those variables, we make some graphs for the categorical columns in relationship with ATTRITION FLAG
for col in categorical_columns:
    fig,ax = plt.subplots(figsize = (8,6))
    ax = sns.countplot(data = data, x = col, palette = "Set2",hue = "Attrition_Flag")
    plt.show()
i) To continue, we are making graphs for categorical columns
for col in categorical_columns:
    fig,ax = plt.subplots(figsize = (8,6))
    ax = sns.countplot(data = data, x = col, palette = "Set2")
    plt.show()
j) If variables have only 1 value, we need to remove them
data[categorical_columns].nunique()
as a result, we find out through function "nunique()" that in our dataset we don t have variables with only one value
k)Encoding
k.1) Rare Labels
values = data["Card_Category"].value_counts()/len(data)*100
data["Card_Category"] = np.where(data["Card_Category"].isin(values.index[2:]),"Superior Cards", data["Card_Category"])
data["Card_Category"].value_counts()/len(data)*100
k.2) Numerical Variables
numerical_columns = [col for col in data.columns if data[col].dtypes != "object" and col!= "Attrition_Flag"]
print("There are", len(numerical_columns),"numerical columns. These are", numerical_columns)
l) Missing value imputation
Description of what we are gonna do: We are going to replace the missing values for the numerical variables with the median value
data["Customer_Age"] = data["Customer_Age"].fillna(data["Customer_Age"].median())
data["Dependent_count"] = data["Dependent_count"].fillna(data["Dependent_count"].median())
data[numerical_columns].isnull().sum()
m) The next step is to look for columns with all elements unique in order to drop them, if there aren't, we don't to anything
data[numerical_columns].nunique()
n)To have a good overview, we should make graphics for numerical columns in relationship with "Attrition_Flag"
for col in numerical_columns:
    fig,ax = plt.subplots(figsize = (15,6))
    sns.histplot(data = data, x = col,hue = "Attrition_Flag",bins = 100)
    plt.show()
OUTLIERS
->Outliers detection
(Check if a column has outliers)
for col in numerical_columns:
    fig,ax = plt.subplots(figsize = (15,6))
    sns.histplot(data = data, x = col,hue = "Attrition_Flag",bins = 100)
    plt.show()
for col in numerical_columns:
    fig,ax = plt.subplots(figsize = (8,6))
    sns.boxplot(y = data[col], color = 'pink', whis = 3)
    plt.show()
We are making a list with all the outliers
heavy_affected_by_outliers = ["Total_Amt_Chng_Q4_Q1","Total_Trans_Amt","Total_Ct_Chng_Q4_Q1"]
def censoring_outliers(dataframe, column):
    q1 = dataframe[column].quantile(0.25)
    q3 = dataframe[column].quantile(0.75)
    IQR = q3 - q1
    lower_limit = q1 - 3 * IQR
    upper_limit = q3 + 3 * IQR
    dataframe[column] = np.where(dataframe[column] < lower_limit, lower_limit, np.where(dataframe[column] > upper_limit,upper_limit,dataframe[column]))
for variable in heavy_affected_by_outliers:
	censoring_outliers(data, variable)
We use a table to have a more accurate visualization of the correlation
correlation = data[numerical_columns].corr()
correlation
We create a Heat Map
fig,ax = plt.subplots(figsize =(24,10))
sns.heatmap(correlation, annot = True, square = True,fmt = ".2f")
plt.show
Because we don't have a very high correlation (+-0.95), we don't have duplicate variables so, we don't drop any columnsÂ¶
MODEL DEVELOPMENT
data.to_csv("C:\\Users\\nafor\\OneDrive\\Desktop\\proiect practica\\transformed_dataset.csv")
data = pd.read_csv("C:\\Users\\nafor\\OneDrive\\Desktop\\proiect practica\\transformed_dataset.csv",index_col ="Unnamed: 0")
We declare the independence variable and the target variable
#independent variables
X = data.drop(columns = ["Attrition_Flag"])

#target variable
y = data["Attrition_Flag"]
X.head()
X["Total_Used_Bal"].nunique()
1974
X = X.drop(columns = ["CLIENTNUM","Total_Used_Bal"])
X.head()
We find out dummy variables
X = pd.get_dummies(X, columns = categorical_columns)
X.head()
Random Forest - Training algorithm
1. We instantiate the model
rf = RandomForestClassifier(n_estimators = 300,max_depth = 5, n_jobs = -1)
2. We train the model
rf.fit(X_train,y_train)
y_predict = rf.predict(X_test)
print(y_predict)
accuracy = accuracy_score(y_test,y_predict)
print(accuracy)
We make the confusion Matrix
cm = confusion_matrix(y_test,y_predict)
fig, ax = plt.subplots(figsize = (12,8))
sns.heatmap(cm,annot = True, fmt = "d")
plt.show()
We get the precision and recall
precision = precision_score(y_test,y_predict)
recall = recall_score(y_test,y_predict)
print("Precision =",precision)
print("Recall =",recall)
To sum up, the model's precision score of 0.969 implies it's proficient at recognizing positive cases, but its recall score of 0.395 suggests it's less competent at identifying all the positive instances.
It's possible that the model is solely recognizing the most conspicuous or straightforward positive instances and failing to identify a significant portion of them.
We get the AUC score
auc_score = roc_auc_score(y_test,y_predict)
print("AUC Score =",auc_score)
With an AUC Score of 0.6966462941138853, the model's performance surpasses that of a random guess, but there is still room for enhancement.
We get the Roc Curve
fpr,tpr,threshold = roc_curve(y_test,y_predict)
plt.plot(fpr,tpr)
If everything was perfect, the ROC curve of a binary classification model would form an ideal right angle by a straight line starting from the bottom left corner, continuing to the top left corner, and then ending at the top right corner.
The hypothetical curve represents a model that flawlessly distinguishes between positive and negative instances, with no instances of false positives or false negatives. In our scenario, the line ascends vertically until reaching 0.4, after which it forms a seamless diagonal line to the top right corner.
We train other algorithm - XGBoost
xgb = XGBClassifier(n_estimators = 300, max_depth = 5,learning_rate = 0.1,n_jobs =-1)
xgb.fit(X_train,y_train)
We predict the results
y_predict = xgb.predict(X_test)
We get the accuracy
accuracy = accuracy_score(y_test,y_predict)
print("Accuracy",accuracy)
We make the confusion Matrix
We get the precision and recall
precision = precision_score(y_test,y_predict)
recall = recall_score(y_test,y_predict)
print("Precision =",precision)
print("Recall =",recall)
To summarize, the model's precision score of 0.927 suggests that it's skillful in accurately identifying positive instances, whereas its recall score of 0.872 indicates that it can detect the majority of positive instances but may overlook a small percentage of them.
Ideally, both precision and recall scores should approach 1 for optimal performance.
We get the AUC score
auc_score = roc_auc_score(y_test,y_predict)
print("AUC Score =",auc_score)
AUC Score = 0.9296854587478645
We get the Roc Curve
fpr,tpr,threshold = roc_curve(y_test,y_predict)
plt.plot(fpr,tpr)
In an ideal situation, the ROC curve of a binary classification model should form a right angle by a straight line, starting from the bottom left corner, continuing to the top left corner, and then concluding at the top right corner, representing a model that flawlessly separates positive and negative examples, without any false positives or false negatives.
However, in our case, the line ascends vertically until reaching 0.83, after which it forms a diagonal line up to the top right corner. This suggests that the model performs well but may misclassify a few instances, leading to a slight deviation from the ideal curve.
We check for Overfitting / Underfitting
y_predict_train = xgb.predict(X_train)
y_predict_test = xgb.predict(X_test)
auc_score_train = roc_auc_score(y_train,y_predict_train)
auc_score_test = roc_auc_score(y_test,y_predict_test)
print("AUC train =",auc_score_train)
print("AUC test =",auc_score_test)
Hyperparameters tuning
n_estimators = [200,300,400,500]
max_depth = [3,5,7 ]
learning_rate = [0.03, 0.05, 0.01]
We search for best hyperparameters
results = []
for est in n_estimators:
    for md in max_depth:
        for lr in learning_rate:
            xgb = XGBClassifier(n_estimators = est,max_depth = md,learning_rate = lr,n_jobs =-1)
            xgb.fit(X_train,y_train)
            y_predict = xgb.predict(X_test)
            auc_score = roc_auc_score(y_test,y_predict)
            results.append(["n_estimators",est,"max_depth",md,"learning_rate",lr,"AUC",auc_score])
The best model seems to be: 'n_estimators'= 500, 'max_depth'= 5, 'learning_rate'= 0.05, 'AUC'= 0.927
We check for overfitting
best_xgb = XGBClassifier(n_estimators = 500,max_depth = 5,learning_rate = 0.05,n_jobs=-1)
best_xgb.fit(X_train,y_train)
y_predict_test = best_xgb.predict(X_test)
y_predict_train = best_xgb.predict(X_train)
auc_score_test = roc_auc_score(y_test,y_predict_test)
auc_score_train = roc_auc_score(y_train,y_predict_train)
print("Auc train =",auc_score_train)
print("Auc test =",auc_score_test)
The model seems to demonstrate satisfactory performance on the training data, but its performance on the test data is suboptimal. This may be due to the model's over-reliance on the training data, causing it to struggle in generalizing to new data.
On the other hand, underfitting occurs when the model is too simplistic and cannot capture the patterns in the data, leading to inadequate performance on both the training and test data.
In summary, based on the information provided, it appears that the model is marginally overfitting.
We save the file
import pickle
with open("C:\\Users\\nafor\\OneDrive\\Desktop\\proiect practica\\best_model_xgb.pkl","wb") as file:
    pickle.dump(best_xgb,file)
y_predict = best_xgb.predict(X_test)
y_predict
y_predict_proba = best_xgb.predict_proba(X_test)
y_predict_proba 
y_predict_proba_class_1 = y_predict_proba[:,1]
y_predict_proba_class_1
sorted_list = sorted(y_predict_proba_class_1, reverse=True)
sorted_list
pr = pd.DataFrame()
pr['proba']=y_predict_proba_class_1
Finally, we found the top 100 most probable clients to churn
top_100 = pr.nlargest(100, 'proba')

print(top_100)
To have a larger overview, those are the top 500 most probable clients to churn
top_500 = pr.nlargest(500, 'proba')

print(top_500)

The top 1000 most proabable clients to churn
top_1000 = pr.nlargest(1000, 'proba')

print(top_1000)
Performance metrics
We check the AUC score for probabilities
auc_score = roc_auc_score(y_test,y_predict_proba_class_1)
print(auc_score)
A score of 0.9927115593681768 indicates that the binary classifier model has a very high discriminatory power, with a very low false positive rate and a very high true positive rate
Lift and Gain Analysis
lift_gain_report = pd.DataFrame()

lift_gain_report["y_test"] = y_test

lift_gain_report["Predicted Probabilities"] = y_predict_proba_class_1

lift_gain_report["Probabilities Rank"] = lift_gain_report["Predicted Probabilities"].rank(method ="first",ascending = True, pct = True)

lift_gain_report["Decile group"] = np.floor((1 - lift_gain_report["Probabilities Rank"]) * 10) + 1

lift_gain_report["Number of observations"] = 1

lift_gain_report = lift_gain_report.groupby(["Decile group"]).sum().reset_index()

lift_gain_report["Cumulative no. of observations"] = lift_gain_report["Number of observations"].cumsum()

lift_gain_report["Cumulative % no of observation"] = lift_gain_report["Cumulative no. of observations"]/lift_gain_report["Cumulative no. of observations"].max()

lift_gain_report["Cumulative no. of positives"] = lift_gain_report["y_test"].cumsum()

lift_gain_report["Gain"] = lift_gain_report["Cumulative no. of positives"]/lift_gain_report["Cumulative no. of positives"].max()

lift_gain_report["Lift"] = lift_gain_report["Gain"] / lift_gain_report["Cumulative % no of observation"]

print(lift_gain_report)
We create a Lift Chart
fix, ax = plt.subplots(figsize = (15,8))
barplot = plt.bar(lift_gain_report["Decile group"], lift_gain_report['Lift'], color = 'purple')
plt.title("Lift bar plot")
plt.xlabel("Decile group")
plt.ylabel("Lift")
plt.xticks(lift_gain_report['Decile group'])

for b in barplot:
    plt.text(b.get_x()+b.get_width()/2, b.get_height()+0.1,round(b.get_height(),2), ha='center')

plt.show()
Lift is a metric that evaluates how much better a model performs compared to a random sample selection. It is computed by dividing the Gain by the proportion of positive samples in the dataset.
Lift values range from 1.000000 to 6.218253, with the maximum value indicating that the model is over six times more effective than a random sample selection.
We create a Gain Chart
lift_gain_report["Random Selection"] = lift_gain_report["Decile group"] / lift_gain_report["Decile group"].max()
fig,ax = plt.subplots(figsize=(12,8))
sns.lineplot(data = lift_gain_report, x = lift_gain_report["Decile group"],y=lift_gain_report["Gain"])
sns.lineplot(data=lift_gain_report,x =lift_gain_report["Decile group"],y=lift_gain_report["Random Selection"])
plt.title("Gain Plot")
plt.xticks(lift_gain_report["Decile group"])
plt.yticks(round(lift_gain_report["Gain"],2))
plt.show()
Gain is a metric that quantifies the increase in the ratio of positive samples as the model's predicted probabilities increase. A Gain value of 1 indicates that the model's performance is comparable to selecting all positive examples.
The Gain values range from 0.623053 to 1.000000, with the maximum value of 1.000000 indicating that the model's predicted probabilities are leading to a high ratio of positive samples.
feat_imp = best_xgb.get_booster().get_score(importance_type = "total_gain")
feat_imp
feature_importance = pd.DataFrame()
feature_importance["Variable"] =feat_imp.keys()
feature_importance["Importance value"]=feat_imp.values()
feature_importance["Importance value %"] = feature_importance["Importance value"] / feature_importance["Importance value"].sum() * 100
feature_importance.sort_values(by = ["Importance value"],ascending = False)
The importance values are sorted in descending order, with "Total_Trans_Ct" having the highest value of 20176.017578 and "Education_Level_College" having the lowest value of 1.836277.
Importance value percentage shows the contribution of each variable to the overall prediction accuracy of the model. From the table, we can conclude that "Total_Trans_Ct," "Total_Trans_Amt," "Avg_Utilization_Ratio," "Total_Ct_Chng_Q4_Q1," and "Total_Relationship_Count" are the most significant variables in forecasting the outcome. Conversely, "Education_Level_College," "Card_Category_Superior Cards," "Card_Category_Silver," and "Marital_Status_Divorced" are the least significant variables.
We make Shap Chart
import shap
explainer = shap.TreeExplainer(best_xgb)
shap_values = explainer.shap_values(X_test)
Using the Gain Analasys we can say that :
Decile 1 contains 10% of the total number of observations, and the cumulative percentage of clients who will leave is 10%, with 62.31% of these clients in Decile 1.
Similarly, Decile 2 contains 10% of the total number of observations, with 111 positive instances and a total of 34% of clients who will churn.
Decile 3 also contains 10% of the total number of observations, with 10 positive instances and a total of 4% of clients who will churn.
As a conclusion 100% of clients that will churn are in the first 3 Deciles, with a total of 321 no. of positives.
Based on the results of the Feature Importance Analysis and the SHAP Chart, it can be concluded that the factors that have the greatest impact on churn are:
- Total_Trans_Ct
- Total_Trans_Amt
- Avg_Utilization_Ratio
- Total_Ct_Chng_Q4_Q1
- Total_Relationship_Count